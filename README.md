<!-- PROJECT LOGO -->
<div align="center">
  <img src="/lip_reading.png" alt="Lip Reading Project Logo" width="600">
  <h3 align="center">ğŸ¤– Automated Lip Reading Using Deep Learning</h3>
  <p align="center">
   An AI-powered system that utilizes deep learning models to analyze lip movements in real-time.
  </p>
</div>
<hr>

<details>
  <summary>ğŸ“‘ Table of Contents</summary>
<br>
- [ğŸ“Œ Project Overview](#-project-overview)
- [ğŸ¯ Features & Objectives](#-features--objectives)
- [ğŸ› ï¸ Technologies Used](#-technologies-used)
- [ğŸ—ï¸ Project Architecture](#-project-architecture)
- [âš™ï¸ Essential Installation Software](#ï¸-essential-installation-software)
- [ğŸ” Expected Output](#-expected-output)
- [âš ï¸ Challenges Faced](#-challenges-faced)
- [ğŸš€ Future Scope](#-future-scope)
- [ğŸ‘¨â€ğŸ’» Author](#-author)
</details>
<hr>

<details>
  <summary>ğŸ“Œ Project Overview</summary>
<br>
The **Lip Reading Project** is an advanced AI-driven system designed to interpret spoken words solely from visual lip movements in video footage. It leverages deep learning models such as **Conv3D** for spatial feature extraction and **LSTM** for sequential analysis, enabling accurate prediction of words based on lip motion. The system processes both real-time and pre-recorded video inputs, making it a versatile tool for speech recognition without audio dependency.  
  
Built with **OpenCV** for video processing and **Streamlit** for an interactive web interface, this project enhances accessibility for individuals with hearing impairments, strengthens security applications, and contributes to AI-powered human-computer interaction. Its scalability and real-time processing capabilities make it a valuable innovation across various domains.  
</details>
<hr>

<details>
  <summary>ğŸ¯ Features & Objectives</summary>
<br>
- **ğŸ§  Deep Learning Model**: Uses a combination of **3D Convolutional Neural Networks (Conv3D)** and **Long Short-Term Memory (LSTM)** networks for accurate lip movement detection.  
- **ğŸ¦» Improved Accessibility**: Aids individuals with hearing impairments by providing an alternative mode of communication.  
- **ğŸ”Š Robust Performance in Noisy Environments**: Functions effectively where traditional speech recognition systems fail.  
- **ğŸ’» Web Application Interface**: Developed using **Streamlit**, allowing users to upload video files and receive textual transcriptions.  
- **ğŸ“¡ Scalability**: The model can be adapted for multiple languages and real-time processing.  
</details>
<hr>

<details>
  <summary>ğŸ› ï¸ Technologies Used</summary>
<br>
- **Programming Language**: Python  
- **Deep Learning Framework**: TensorFlow, Keras  
- **Computer Vision**: OpenCV  
- **Model Architecture**: CNN-LSTM (Conv3D + LSTM)  
- **Data Processing**: NumPy, Pandas  
- **Visualization**: Matplotlib  
- **Web Framework**: Streamlit  
</details>
<hr>

## ğŸ—ï¸ Project Architecture  
<details>
  <summary>Click here to expand/collapse</summary>

  <details>
    <summary>ğŸ“Š Data Collection & Preprocessing</summary>
<br>
   - Downloaded video datasets with labeled speech.  
   - Extracted **frames from video** and converted them into grayscale images.  
   - Applied **lip detection and cropping** techniques to focus on the mouth region.  
   - Normalized image data and converted it into an array for model training.  

  </details>

  <details>
    <summary>ğŸ§  Model Development</summary>
  <br>
   - **Conv3D Layers**: Extract spatial and temporal features from video frames.  
   - **MaxPooling Layers**: Reduce dimensionality for computational efficiency.  
   - **LSTM Layers**: Learn sequential patterns in lip movements.  
   - **Dense Layers**: Convert extracted features into text predictions.  
   - **CTC Loss Function**: Used for alignment-free speech recognition.  

  </details>

  <details>
    <summary>ğŸ“ˆ Training & Evaluation</summary>
    <br>
   - The model was trained on a dataset of lip movements and corresponding text transcripts.  
   - **Performance Metrics**: Accuracy, Precision, Recall, and WER (Word Error Rate).  
   - Data split into **80% training and 20% testing** for model validation.  

  </details>

  <details>
    <summary>ğŸŒ Web Application (Streamlit)</summary>
    <br>
   - Built an **interactive UI** where users can upload a video and receive real-time transcription.  
   - Used pre-trained models to predict text from uploaded videos.  
   - Displayed **frame-wise visualization** of lip movement predictions.  

  </details>

</details>
<hr>

<details>
  <summary>âš™ï¸ Essential Installation Software</summary>
<br>
Ensure you have the following installed:  

- Python 3.8+  
- TensorFlow 2.x  
- OpenCV  
- Streamlit  
- NumPy  
- Pandas  
- Matplotlib  
- Imageio  
- Gdown  
</details>
<hr>

<details>
  <summary>ğŸ” Expected Output</summary>
<br>
- The model will display a **sequence of predicted words** corresponding to the lip movements.  
- Accuracy will depend on **lighting conditions, speaker clarity, and dataset quality**.  
</details>
<hr>

<details>
  <summary>âš ï¸ Challenges Faced</summary>
<br>
- **ğŸ“‰ Dataset Limitations**: Lip-reading datasets are limited and require significant preprocessing.  
- **ğŸ’» Computational Intensity**: Training Conv3D and LSTM models requires high GPU power.  
- **ğŸ‘„ Speaker Variability**: Different individuals have unique lip movement styles, affecting model accuracy.  
</details>
<hr>

<details>
  <summary>ğŸš€ Future Scope</summary>
<br>
- Implement **real-time lip reading** for live video streams.  
- Expand the dataset to support **multiple languages and accents**.  
- Optimize the model to work efficiently on **edge devices** like mobile phones.  
- Enhance the accuracy using **transformer-based architectures**.  
</details>
<hr>


<div align="center">
  <p>ğŸ’¡ Developed by <strong>Madhav</strong></p>
  <p>ğŸ“¬ Feel free to reach out for questions or contributions!</p>
  <p>ğŸš€ Happy Coding!</p>
</div>

