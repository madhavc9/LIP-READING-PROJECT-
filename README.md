<!-- PROJECT LOGO -->
<div align="center">
  <img src="/lip_reading.png" alt="Lip Reading Project Logo" width="600">
  <h3 align="center">ü§ñ Automated Lip Reading Using Deep Learning</h3>
  <p align="center">
   An AI-powered system that utilizes deep learning models to analyze lip movements in real-time.
  </p>
</div>
<hr>

## üìë Table of Contents  
<details>
  <summary>‚ñ∂Ô∏è Table of Contents</summary>

- [‚ñ∂Ô∏è Project Overview](#-project-overview)
- [‚ñ∂Ô∏è Features & Objectives](#-features--objectives)
- [‚ñ∂Ô∏è Technologies Used](#-technologies-used)
- [‚ñ∂Ô∏è Project Architecture](#-project-architecture)
  - [‚ñ∂Ô∏è Data Collection & Preprocessing](#data-collection--preprocessing)
  - [‚ñ∂Ô∏è Model Development](#model-development)
  - [‚ñ∂Ô∏è Training & Evaluation](#training--evaluation)
  - [‚ñ∂Ô∏è Web Application (Streamlit)](#web-application-streamlit)
- [‚ñ∂Ô∏è Essential Installation Software](#-essential-installation-software)
- [‚ñ∂Ô∏è Expected Output](#-expected-output)
- [‚ñ∂Ô∏è Challenges Faced](#-challenges-faced)
- [‚ñ∂Ô∏è Future Scope](#-future-scope)
- [‚ñ∂Ô∏è Author](#-author)
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Project Overview</summary>

The **Lip Reading Project** is an advanced AI-driven system designed to interpret spoken words solely from visual lip movements in video footage. It leverages deep learning models such as **Conv3D** for spatial feature extraction and **LSTM** for sequential analysis, enabling accurate prediction of words based on lip motion. The system processes both real-time and pre-recorded video inputs, making it a versatile tool for speech recognition without audio dependency.  

Built with **OpenCV** for video processing and **Streamlit** for an interactive web interface, this project enhances accessibility for individuals with hearing impairments, strengthens security applications, and contributes to AI-powered human-computer interaction. Its scalability and real-time processing capabilities make it a valuable innovation across various domains.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Features & Objectives</summary>

- **üß† Deep Learning Model**: Uses a combination of **3D Convolutional Neural Networks (Conv3D)** and **Long Short-Term Memory (LSTM)** networks for accurate lip movement detection.  
- **ü¶ª Improved Accessibility**: Aids individuals with hearing impairments by providing an alternative mode of communication.  
- **üîä Robust Performance in Noisy Environments**: Functions effectively where traditional speech recognition systems fail.  
- **üíª Web Application Interface**: Developed using **Streamlit**, allowing users to upload video files and receive textual transcriptions.  
- **üì° Scalability**: The model can be adapted for multiple languages and real-time processing.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Technologies Used</summary>

- **Programming Language**: Python  
- **Deep Learning Framework**: TensorFlow, Keras  
- **Computer Vision**: OpenCV  
- **Model Architecture**: CNN-LSTM (Conv3D + LSTM)  
- **Data Processing**: NumPy, Pandas  
- **Visualization**: Matplotlib  
- **Web Framework**: Streamlit  
</details>
<hr>

## üèóÔ∏è Project Architecture  

<details>
  <summary>‚ñ∂Ô∏è Data Collection & Preprocessing</summary>

- Downloaded video datasets with labeled speech.  
- Extracted **frames from video** and converted them into grayscale images.  
- Applied **lip detection and cropping** techniques to focus on the mouth region.  
- Normalized image data and converted it into an array for model training.  
</details>

<details>
  <summary>‚ñ∂Ô∏è Model Development</summary>

- **Conv3D Layers**: Extract spatial and temporal features from video frames.  
- **MaxPooling Layers**: Reduce dimensionality for computational efficiency.  
- **LSTM Layers**: Learn sequential patterns in lip movements.  
- **Dense Layers**: Convert extracted features into text predictions.  
- **CTC Loss Function**: Used for alignment-free speech recognition.  
</details>

<details>
  <summary>‚ñ∂Ô∏è Training & Evaluation</summary>

- The model was trained on a dataset of lip movements and corresponding text transcripts.  
- **Performance Metrics**: Accuracy, Precision, Recall, and WER (Word Error Rate).  
- Data split into **80% training and 20% testing** for model validation.  
</details>

<details>
  <summary>‚ñ∂Ô∏è Web Application (Streamlit)</summary>

- Built an **interactive UI** where users can upload a video and receive real-time transcription.  
- Used pre-trained models to predict text from uploaded videos.  
- Displayed **frame-wise visualization** of lip movement predictions.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Essential Installation Software</summary>

Ensure you have the following installed:  

- Python 3.8+  
- TensorFlow 2.x  
- OpenCV  
- Streamlit  
- NumPy  
- Pandas  
- Matplotlib  
- Imageio  
- Gdown  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Expected Output</summary>

- The model will display a **sequence of predicted words** corresponding to the lip movements.  
- Accuracy will depend on **lighting conditions, speaker clarity, and dataset quality**.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Challenges Faced</summary>

- **üìâ Dataset Limitations**: Lip-reading datasets are limited and require significant preprocessing.  
- **üíª Computational Intensity**: Training Conv3D and LSTM models requires high GPU power.  
- **üëÑ Speaker Variability**: Different individuals have unique lip movement styles, affecting model accuracy.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Future Scope</summary>

- Implement **real-time lip reading** for live video streams.  
- Expand the dataset to support **multiple languages and accents**.  
- Optimize the model to work efficiently on **edge devices** like mobile phones.  
- Enhance the accuracy using **transformer-based architectures**.  
</details>
<hr>

<details>
  <summary>‚ñ∂Ô∏è Author</summary>
<div align="center">
  <p>üí° Developed by <strong>Madhav</strong></p>
  <p>üì¨ Feel free to reach out for questions or contributions!</p>
  <p>üöÄ Happy Coding!</p>
</div>
</details>
